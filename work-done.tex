\chapter{Work Done}

 
\section{Implementation of MTCNN Face Detection on Raspberry Pi}
\label{mtcnn_face}
\subsection{Setup and Testing}\\[.2in]
We implemented the MTCNN code from previous project \cite{GitMTCNN}. The code was upgraded from Python2.7 to Python3 to cope with the up-to-date develop environment on Raspberry Pi 3B and 4.\\[.2in]
Upon successful environment setup and some correction on original code (to fix the false positive detection issue), we run the program on Raspberry Pi which takes the data stream from the camera module. The result turns out to be unsatisfying. There are two main issues:
\begin{itemize}
    \item Slow framerate: FPS of 0.1 or less.
    \item Additional Delay. An extra constant delay is present in addition to the processing delay.
\end{itemize}
The model can achieve almost 100\% accuracy in our lab camera testing scenario. A significant pitfall of this model, however, lies in the processing-time. Even if on the Macbook Pro (Specs in section  \ref{mac}), the model (via {\it Caffe}) can only achieve 5 FPS (frame per second), way less than the FPS that satisfies the real-time standard (about 30 FPS). On the Raspberry Pi 3B (Specs in section \ref{rp3}), the FPS is about 0.1 or less.

\subsection{Proposed solutions}\\[.2in]

We proposed a few routes to improve the situation:
\begin{enumerate}
    \item Tune camera resolution and other settings;
    \item Pre-processing the input image (Gray-scale);
    \item Use PiCamera read function instead of OpenCV function to rule out buffer delay.
    \item Utilise the GPU on Raspberry Pi;
    \item Utilise all the CPU cores on Raspberry Pi;
    \item Design a way to search the image pyramid to avoid brute-force iteration;
\end{enumerate}
Unfortunately, the GPU (Specs in section \ref{rp3}) on Raspberry Pi 3B cannot be utilised by existing deep learning packages; hence, this approach will not be discussed below. The improvement of CPU utilisation will be discussed in the next section.

\subsubsection{Resizing the input image and Gray-scaling the input picture}\\
The detection with the input image size of $320\times240$ can achieve approximately 2 FPS when dealing with single face. In light of that camera settings may affect the performance. We go about exploring how the colour channels affect the performance. To experiment, we convert a number of picture to Black and White and compare the processing time and results with the original pictures. The improvement in processing time is significant, presumably by reducing the number of bounding boxes produced by P-Net, as illustrated in Figure \ref{Figure.color}: \\[.2in]
\begin{figure}
    \centering
      \subfloat{\includegraphics[width=.3\linewidth]{color.png}}\qquad
      \subfloat{\includegraphics[width=.3\linewidth]{bw.png}}
      \label{Figure.color}
      \caption{Coloured image (P-Net: 72 bounding boxes) and Black-White image (P-Net: 37 bounding boxes) results}
\end{figure}
Drastic improvement of about 40\% in run time (from 2.25s to 1.28s) can be seen in the experiment. Gray-scaling the input has a significant effect on reducing candidate boxes, thereby reducing the total run time. It is also good to learn that the detection results do not rely too much on the colour space.

\subsubsection{Using threading to decrease the I/O latency and consequently increase FPS:}\\
A setup delay of about 3s is noteworthy during our test. When the camera was released and re-initialized every time individual frame has been shown, the setup delay is found reduced.\\[.2in]
Therefore, we assume the setup delay is from the camera buffer from \verb#OpenCv# module. That is, the program only takes the picture at the front of the buffer, which is taken about 2 seconds ago. Together with the processing time, we see a delay of about 3 seconds. We try to modify the buffer size to mitigate this delay. \\[.2in]
We previously used \verb#cv2.VideoCapture# but the real-time output recording was slow and sluggish because in this method, reading and decoding the frame happened both in the main processing thread. The \verb#.read()# method is a blocking operation. The main thread of Python and \verb#OpenCv# operation is entirely blocked until the frame is read from the camera, decoded, and returned to the calling function. Therefore, we move those blocking I/O operations to a separate thread and maintain a queue of decoded frames. Finally, The main thread is freed to do the actual information processing.\\[.2in]
Instead, we apply \verb#PiCamera# using threading. The setup delay is reduced to a negligible level.

\subsubsection{Reducing Image Pyramid by setting selected scale}\\

We noticed that the image pyramid in our real-time face detection is inefficient. Although, it can cope with different sizes of faces in images with a wide range of sizes, however, in our case, the camera takes a picture with a fixed size ($320\times240$) at a fixed position. Also, it is noticeable that the bottleneck so far is at P-Net. Therefore, we consider reducing the image pyramid and defining scales manually to reduce the image (sliding boxes) at P-Net.\\[.2in]
The scale list of [0.4, 0.148, 0.128, 0.1, 0.08] works well. It reduces processing time for an individual frame when a single face appears to 0.27s (3.7 fps) and remains a relatively wide range of detection distance.

\subsection{Multi-core utilisation}\\[.2in]
The idea is based on the observation that the video FPS is significantly higher than that of the detection. They are referred to as ‘Display FPS’ and ‘Detection FPS’ respectively. The current code’s logic is such that a frame is taken only when the previous frame has been processed by the network and displayed with resultant boxes marked. This does not fully utilise the upper-bound of display FPS and creates pauses in the final display. In addition, the Global Interpreter Lock (GIL) of Python prevents the use of multiple cores; therefore, our computing power is restricted to 1 core while there are 4 cores on board.\\[.2in]
Based on what is said above, we propose that a single process be responsible for the camera I/O and the other three collaborate in the detection task. Core 0 is reading and putting the frame into a shared queue and retrieving results from another queue in which detection boxes are stored. It checks the detection queue and updates if there is any result in the queue. If there is no detection result available, it will mark the latest detection boxes. \\[.2in]
The detection processes do their best to clear the input queue. The main bulk of the program is identical to the previous program, which inputs the image into the network and gets results. The only difference is that the input comes from the input queue filled by the I/O process, and the output is put into the output queue. \\[.2in]
This way, we expect a high-fps display with delayed bounding boxes. We can also make use of all the CPU available as well. This should increase our computing power noticeably.Pseudo-code is as follows:\\[.2in]
Main (I/O) process:\\[.2in]
\verb#from multiprocessing import Process, Queue#\\
\verb#Initialise input_q and output_q as Queue#\\
\verb#Detection_p1,2,3 = Process(target=detection_task, #\\
\verb#                           args=input_q, output_q)#\\
\verb#Detection_p1,2,3.start()#\\[.2in]
\verb#While True:#\\
\verb#  Frame = getFrame()#\\
\verb#  input _q.put(Frame)#\\[.2in]
\verb#  If output_q is not empty:#\\
\verb#    Bounding_box = output_q.get()#\\
\verb#  Frame = drawBox(Frame, bounding_box)#\\
\verb#  Frame.display()#\\[.2in]
Detection process:\\[.2in]
\verb#Initialisation#\\[.2in]
\verb#While True:#\\
\verb#  If input_q is not Empty:#\\
\verb#    Frame = input_q.get()#\\
\verb#    Bbox = detect_face(Frame)#\\
\verb#    Output_q.put(bbox)#\\[.2in]
The test suggests slight improvement on detection FPS which is still not satisfactory. The limited improvement is possibly from low efficiency of read and write procedures among CPU cores. 
\subsection{Optimisation: Detection Scheme for real-time detection}
Given that the video is continuous in time if the camera frame rate is reasonably fast, it is likely to find the face in the proximity of its last appearance. Therefore we can strip out the redundant background and compress the input size to the network, which we identified as our bottleneck for higher speed.\\[.2in]
In abstract terms, a searcher and multiple trackers will be responsible for detection. The searcher runs face detection on the whole image and should be capable of detecting the face of any size in a given image. Based on what is found by the searcher, a tracker is spawned to focus on the proximity of each positive detection (called window). Trackers only feed the network with this windowed image so we can expect much faster processing speed. The tracker then tracks the face and update its bounding box until it loses it. In other words, the searcher provides a heuristic for the trackers on where to look for a face. Then the tracker is responsible for tracking the face.\\[.2in]
Pseudocode is as follows:\\[.2in]
\verb#Initialize processes, cam, etc.#\\[.2in]
\verb#While True:#\\
\verb#  Frame = cam.capture()#\\
\verb|  Det_resutlt = output_queue.get_all_data()|\\
\verb|  # contains detection results and id |\\[.2in]
\verb#  For item in det_result:#\\
\verb#    If item.id == 0:#\\
\verb#      For box in item.data, if IoU(box, #\\
\verb#                  all trackers result b0x)<threshold#\\
\verb#        Spawn tracker with box#\\
\verb#      Else:#\\
\verb#        Update corresponding tracker with item.data and frame#\\
\verb#  If searcher completes one detection:#\\
\verb#    input_queue.put((frame,0))#\\[.2in]
\verb#  For t in trackers:#\\
\verb#    If t.sent_flag is False:#\\
\verb#      input_queue.put(t.get_window_img(),t.id)#\\
\verb#        t.set_sent_flag()#\\[.2in]
\verb#  Boundingbox = []#\\
\verb#  For t in trackers:#\\
\verb#    boundingbox.append(t.get_result_bbox())#\\
\verb#  Img = drawBox(frame, boundingbox)#\\
\verb#  cv2.imshow(Img)#\\[.2in]
The program can run three trackers at an FPS of around 5 with a smooth video display. More trackers may slow down the detection FPS, but the overall performance has been improved to a great extent. We might further reduce the processing time feeding the inputs directly to R-Net or O-Net. 
\subsubsection{Further improvement on detector scale}
As aforementioned, the tracker takes the candidate image cut from the original image according to the previous detection result (bounding box). In this process, the tracker will take a slightly larger candidate image than the bounding box (at a scale of 1.2 in our version). In this way, we consider there exist certain scales that we can use to substitute the image pyramid for candidate images, which can significantly boost efficiency. \\[.2in]
We adjust the ratio testing on a single frame and find out that it works well by sending image window with the smallest edge of 20 pixels length and using the scale of 0.75 (after the factor of 0.6). Without affecting its accuracy, the FPS achieves 10 on Raspberry Pi 3B (Specs in section \ref{rp3}) and 20 on Raspberry Pi 4 (Specs in section \ref{rp4}).

\section{Attempt on MTCNN Hand Gesture Detection}
Gesture Recognition has been the hot-spot for research in recent years. The efforts are mainly put into building a gesture recognition system which can augment human-computer interaction. Recent approaches include conventional computer vision, deep learning, and a hybrid of the two. 
\subsection{Ideas and Objectives}
Gesture recognition bears many similarities with face recognition. Both need to locate the object and identify certain features of the object. Therefore, it is reasonable to employ MTCNN which is designed for face recognition in gesture recognition tasks.\\[.2in]
The system consists of three individual networks, Proposal (P-Net), Refine (R-Net) and Output (O-Net) Net. They vary in the input size and the number of neurons and hidden layers, but the general architectures are CNN for all. The output layer consists of 15 neurons, among which one of them indicates whether a hand is presented, 4 are for bounding box regression, and the remaining 10 for gesture recognition.\cite{7553523}\\[.2in]
This design assumes that the task of detecting a hand is intrinsically helpful for gesture recognition; therefore, they may share the same features. This is an analogy of face detection useful for face alignment for the original MTCNN project. Besides, we have shown in last section \ref{mtcnn_face} that such a network can be run on a Raspberry Pi. \\[.2in]
For the sake of speedy development, instead of using an existing database, we are going to build our database which should be lightweight and accessible.
\subsection{Adjustment}
The MTCNN network used for face detection can be adapted to gesture recognition for the following reasons:
\begin{enumerate}
    \item Locating the hand is a vital task;
    \item The network can be run real-time;
    \item Recognising the hand is likely to have a positive impact on gesture recognition
\end{enumerate}
We decided to experiment with an existing MTCNN-tensorflow project \cite{PytorchMTCNN} to train our gesture detection model. We have only modified the output layer to fit our 3-gesture classifier. Specifically, the landmark neurons, originally 10 in total, are reduced to three with a softmax layer connected to them. The structure of the network is identical to that in the paper \cite{7553523}.\\[.2in]
In the training stage, it is noticed that the additional task of gesture classification may not benefit the tasks of bounding boxes for its abstraction compared with the direct geometric task in face detection. We find that, without the classification task, the loss converges in fewer epochs.\\[.2in]
Building on that, we decided to build an MTCNN detector first, and then through a shallow CNN net to determine its gesture given the hand image.
\subsection{Dataset}
\subsubsection{SCUT-Ego-Gesture dataset}
SCUT-Ego-Gesture dataset is the extension of SCUT-Ego-Finger dataset, containing 59,111 RGB images in the egocentric vision (Figure \ref{Fig.main3}) with 16 different hand gestures. Hand gestures used in our project include SingleOne, SingleTwo, SingleFour, SingleSix,  SingleEight, SingleNine, SinleBad and SingleGood; and other 5 gestures with both hands\cite{ego}.\\[.2in]
A downside of this dataset is that it is not various enough for hand detector and biased for hand classification (mostly back-hand images).
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{example1.png}
\caption{SCUT-Ego-Gesture dataset sample images \cite{ego}} 
\label{Fig.main3}
\end{figure}
In our project, we only use its bounding boxes information to locate the hand as an input of MTCNN. Fingertip landmarks are considered to be added at the later stage of the project. 
\subsubsection{Self-collected testing Dataset}
We have also collected an original dataset (Figure \ref{Fig.main4}) with the gesture ``one", ``two", ``fist" and ``others" to test our network. The dataset was collected under various light conditions and in versatile environments, including lab, lecture hall and common study rooms.

\begin{figure}
\centering
\vspace{.1in}
\includegraphics[width=1\textwidth]{dataset.png}
\caption{Collected dataset sample images}
\label{Fig.main4}
\end{figure}

\subsection{Problems encountered and possible solutions}
\subsubsection{Variety of Hand Shapes}
One notable difference between face detection and hand detection is the variety of shapes. The images given to the detector can be from multiple angles, which increases the difficulty of building a robust model and demands larger dataset. Given the existing application of gesture detection, most dataset offers images via infrared sensors, which causes a limitation of our sample source.\\[.2in]
In conclusion, massive datasets with more gestures are preferable to avoid over-fitting and enable a better generalisation. 
\subsubsection{Unstable Detection}
Our first model gave an unstable real-time camera testing performance, which was reflected by the swaying of the bounding box marked. This phenomenon is quite normal, even with stationary hands. \\[.2in]
By analysing the inference results, we can see that in some cases like Figure \ref{Fig.test2} and Figure \ref{Fig.test3}, the bounding boxes are too large or deviate from the correct place. Although the bottom shape of the gesture ``one" is similar to the gesture ``fist", we still expect the detector to distinguish them in a sense.\\[.2in]
The swaying of bounding boxes is also believed due to an overly sensitive model, which means a slight change of input image significantly influences the prediction results. We consider that the augmentation of training data, such as exposure, hue, will mitigate this problem.\\[.2in]
This problem can also result from a non-optimised network. After some tuning of hyper-parameters, the true positive rate increases. We notice a decrease in swaying under same circumstances.

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{4.png}
\caption{Correct bounding box} 
\label{Fig.test1}
\vspace{.2in}
\includegraphics[width=.8\textwidth]{12.png}
\caption{Large bounding box} 
\label{Fig.test2}
\vspace{.2in}
\includegraphics[width=.8\textwidth]{21.png}
\caption{Deviated bounding box} 
\label{Fig.test3}
\vspace{.2in}
\includegraphics[width=.8\textwidth]{80.png}
\caption{False Negative Detection} 
\label{Fig.test4}
\end{figure}
\subsubsection{Decreasing training data}
The training processes for MTCNN networks are also cascaded, which means the training data for R-Net are generated from a well-trained P-Net; O-Net takes data from R-Net as well. The cascaded structure suggests a decreasing training sample. In our case, the training sample for P-Net and R-Net can achieve 300000 to 500000 images while O-Net can only get about 60000, nearly 1/10 of the previous two.\\[.2in]
This fact has resulted in a weakness in O-Net training compared with the other two. Considering the networks are designed for a process of refining, R-Net and O-Net should be robust. In the future research, the influence and feasibility of using training data from P-Net can be explored. 
\subsection{Analysis of Previous Model}
From previous successful model of Face Detection, it is noticeable that the proposal of bounding boxes from P-Net is rough (Figure \ref{Fig.model1}), which contains many false positive results. This makes sense due to its shallow network, and is sufficiently accurate for R-Net. \\[.2in]
R-Net has a robust ability of detecting and correcting bounding boxes. Although the images fed into R-Net are cropped from original images, R-Net is able to compensate possible missing of parts and give deviation corrections (Figure \ref{Fig.model2}). \\[.2in]
Inspired by this, the priority of our training goes to tune the hyper-parameters for a reasonably good P-Net and a robust R-Net given the significance of R-Net in the MTCNN.
\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{previous-P.png}
\caption{Face detection result of P-Net and O-Net} 
\label{Fig.model1}
\vspace{.4in}
\includegraphics[width=.9\textwidth]{previous_model.png}
\caption{Demonstration of Face detection results of P-Net, R-Net} 
\label{Fig.model2}
\end{figure}
\subsection{Training Result}
\subsubsection{MTCNN Hand Detection}
1.	Tuning Learning Rate and learning rate ratio\\[.2in]
The learning rate, in our application, will change with the epoch number, which is defaulted as [6,14,20]; for example, learning rate will be multiplied by the learning rate factor at the given epochs. For different databases and networks of the model, we need to apply different learning rate parameters, which are obtained by experiments. Among our trials, we have encountered the following problems:
\begin{itemize}
    \item (Figure \ref{Figure.1-1}) The accuracy figure does not climb up or the loss figure does not fall to a satisfactory level while the curve seems relatively smooth and steady. In this scenario, we consider lifting the base learning rate by one order of magnitudes in next run.
    \item (Figure \ref{Figure.2-2}) The accuracy figure and the loss figure does not appear convergent and the graph shows intensive turbulence. In this scenario, we consider decreasing the base learning rate by one order of magnitudes in next run.
    \item (Figure \ref{Figure.3-3}) Different part of the graph shows different features and tendency. We consider base learning rate, learning rate factor together with the learning rate change epoch number and adjust them accordingly.
\end{itemize}
For instance, while training the O-Net, we experimented with different combinations of base learning rate and learning rate factor, and three of them are listed here: \\[.2in]
\begin{figure}
    \centering
      \subfloat{\includegraphics[width=.4\linewidth]{1-1.png}}\qquad
      \subfloat{\includegraphics[width=.4\linewidth]{1-2.png}}
      \label{Figure.1-1}
      \caption{Case 1: Base learning rate = 0.01; Learning rate factor = 0.1; Loss diagram (left) and Accuracy diagram (right)}
\end{figure}
\begin{figure}
    \centering
      \subfloat{\includegraphics[width=.4\linewidth]{2-1.png}}\qquad
      \subfloat{\includegraphics[width=.4\linewidth]{2-2.png}}
      \label{Figure.2-2}
      \caption{Case 2: Base learning rate = 0.1; Learning rate factor = 0.1; Loss diagram (left) and Accuracy diagram (right)}
\end{figure}
\begin{figure}
    \centering
      \subfloat{\includegraphics[width=.4\linewidth]{3-1.png}}\qquad
      \subfloat{\includegraphics[width=.4\linewidth]{3-2.png}}
      \label{Figure.3-3}
      \caption{Case 3: Base learning rate = 0.1;Learning rate factor = 0.5; Loss diagram (left) and Accuracy diagram (right)}
\end{figure}
2.	Sample ratio control:\\[.2in]
Training samples are divided into three domains: Positive, Negative, Partial. They are all generated by randomly cropping boxes around the ground truth bounding box and grouped according to IoU (overlap area over total area). There are 2 dimensions for ratio control:
\begin{enumerate}
    \item For the whole dataset: the portion of the three kinds of samples are defaulted as 1:3:1 for P-Net and R-Net, 1:1:1 for O-Net. To control the samples ratio in the whole dataset, we adjust the number of samples according to the ratio before generating \verb#tf-records#. We try to adjust the ratio according to the false positive rate and false negative rate detected in the inference stage. \\[.2in] It is also noteworthy that the training samples are decreasing through the training process, and when it comes to O-Net there might only be few negative samples, which is significantly outnumbered by other two kinds of samples. We chose to keep all the samples generated from previous networks and receive the result in Figure \ref{Fig.4-4}.\\[.2in] During the inference using images and camera captures, the performance of the whole network also saw some improvement. 
    \item For each batch: the intake sample batch for training will contain specified portions of positive, negative or part samples. With relatively consistent ratio of sample kinds, large fluctuation during training is less likely to happen. And this could also impact the learning outcome. For next stage we may try to experiment its influence on the training outcome of different nets.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{4-1.png}
\caption{Total loss diagram for O-Net Training} 
\vspace{.4in}
\includegraphics[width=.8\textwidth]{4-2.png}
\caption{Accuracy for O-Net Training} 
\label{Fig.4-4}
\end{figure}

\subsubsection{Hand Gesture Classification}

The CNN used for hand gesture classification has a structure similar to the hand-writing number recognition using MINST \cite{lecun-mnisthandwrittendigit-2010} (Modified National Institute of Standards and Technology database).\\[.2in]
The Network takes input grey scale images in $28\times28$, takes $3\times3$ 2D Convolution with 32 kernels, and then $3\times3$ 2D Convolution with 64 kernels. Flatten the tensor afterwards. Connect it to a Dense layer of 128 neurons, after which connect it to a Dense layer of 8 neurons which correspond to the length gesture class list (Figure \ref{Fig.cnn}).
\begin{figure}
\centering
\includesvg[width=.6\textwidth]{nn.svg}
\caption{Diagram for CNN Classification Network}
\label{Fig.cnn}
\end{figure}
The training images are gained from ego-gesture dataset \cite{ego}, which are cropped from original images, grey-scaled and resized to $28\times28$ (Figure \ref{Fig.cnn1}).\\[.2in]
Within 20 epochs, the accuracy on training images are achieving 97\%. The accuracy on testing samples can also get to 94.5\% (Sample output: Figure \ref{Fig.cnn2}). However, it is noteworthy that some images in the dataset are of high similarity, hence, the testing data may be similar to what have been trained.\\[.2in]
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{cnn_train.png}
\caption{Training Input Images} 
\label{Fig.cnn1}
\vspace{.4in}
\includegraphics[width=1\textwidth]{cnn_results.png}
\caption{Some inference results of CNN} 
\label{Fig.cnn2}
\end{figure}
The real-time camera test gives an unsatisfactory results due to salient difference between training dataset (back camera) and testing environment (front camera). The following solutions are proposed to tackle this problem:
\begin{enumerate}
    \item Augment the dataset (Flip horizontally, Random deviation)
    \item To cope with small deviations from O-Net, crop the image slightly larger than the bounding box.
\end{enumerate}



